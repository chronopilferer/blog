---
title: "DeepSeek-R1"
date: "2025-03-15"
category: "Paper Review"
tags: ["AI", "DL"]     
summary: "순수한 강화학습의 가능성"
image: "images/deepseek-logo.png"
---

# DeepSeek-R1

최근 중국에서 **DeepSeek-R1**이라는 언어 모델이 개발되었다.  
OpenAI의 o1 모델에 견줄 수 있는 성능, 압도적으로 낮은 개발 비용, 단 2달이라는 짧은 개발 기간, 그리고 **H800 GPU 기반의 학습**이라는 사실이 공개되며, 글로벌 AI 생태계에 큰 반향을 일으켰다.  

이 발표 이후, **고성능 칩 없이도 고성능 AI가 가능하다**는 논의가 확산되며, NVIDIA를 포함한 여러 빅테크 기업의 주가가 하락하기도 했다.  
물론, SemiAnalysis 측은 “해당 비용에는 사전학습 외의 R&D, 인프라, 운영 비용이 빠졌을 수 있다”고 언급했지만, 논문과 학습 코드를 모두 공개한 만큼 **허위는 아닐 것**이라는 반론도 존재한다.  

이번 포스트에서는 해당 모델의 기반이 되는 논문인    
**DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning**을 소개하며,  
DeepSeek-R1이 기존 모델들과 어떤 차별성을 가지는지, 어떤 학습 전략을 사용했는지 정리해보았다.  

# Summary

이 논문은 대규모 언어 모델(LLM)의 **추론 능력을 강화학습(RL)만으로 향상시킬 수 있다**는 가설을 검증하였다.  
기존에는 대부분 지도 미세 조정(SFT)을 기반으로 한 후 RL을 수행했지만, 이 연구는 **SFT 없이 순수한 RL만으로도 모델이 스스로 추론 능력을 발달시킬 수 있음**을 보였다.

또한 **지식 증류(distillation)를 통해 대형 모델의 추론 능력을 소형 모델에게 효과적으로 전달할 수 있다는 점**도 입증하였다. 

논문에서 제시된 핵심 모델은 다음과 같다:

## 1. DeepSeek-R1-Zero  

첫째로 Zero 모델이다  

이 모델은 어떠한 지도 학습 데이터도 주지 않고 순수 강화 학습만을 사용해 추론 능력을 향상시키는 것을 보여줬다.  

훈련 과정을 보면 놀라운데 모델 스스로 **사고 시간(추론 토큰 수)** 을 스스로 늘리며 복잡한 추론 능력을 자연스럽게 향상시켰다는 점이다.
테스트 시간 연산이 증가함에 따라 반성이나 대체 접근법 탐색 같은 **고도화된 사고 전략** 을 외부 지시 없이 자발적으로 판단하고 발전했다.  

즉, 명시적으로 모델에게 문제를 해결하는 방법을 지시하는 것이 아니라 **올바른 보상** 을 제공하는 것만으로도 고급 문제 해결 전략을 세울 수 있도록 할 수 있다.

하지만, 이 과정에서 **출력의 가독성이 떨어지거나, 다국어가 혼합되는 현상** 같은 부작용도 발생했다.

## 2. DeepSeek-R1

위 문제를 보완하기 위해 DeepSeek 연구팀은 소량의 고품질 **CoT(Chain-of-Thought) 데이터를 활용한 콜드스타트**와 함께, **SFT와 RL을 결합한 DeepSeek-R1 모델**을 설계했다.  

세부적인 단계는 다음과 같다.  

### 2-1 Cold Start  

초기 학습의 불안정성을 줄이기 위해, 수천 개의 고품질 Chain-of-Thought (CoT) 데이터를 수작업으로 구성해 모델을 먼저 미세 조정했다.  
이로써 추론 구조의 틀을 사전에 익혀 안정적인 출발이 가능해졌다.  

### 2-2 Reasoning-oriented RL

단순한 문장 생성이 아니라, 수학 문제나 코딩 과제처럼 복잡한 논리 구조를 스스로 학습하도록 추론 특화 보상 함수를 활용해 강화 학습을 수행했다.

### 2-3 Supervised Fine-Tuning

강한 추론 능력 외에도, 작문, 번역, 요약 등 다양한 실제 시나리오에 대응할 수 있도록 RL 이후 Supervised Fine-Tuning을 통해 범용성과 실용성을 확보했다.

### 2-4 RL for All Scenarios

마지막 단계에서는 **helpfulness(유용성), harmlessness(무해성), human alignment(인간 친화성)** 을 고려한 RL을 적용해, 보다 신뢰할 수 있는 사용자 친화적 모델로 재정비했다.  

---

이렇게 다단계 학습 과정을 거친 DeepSeek-R1은 추론 능력, 문제 해결력, 그리고 인간과의 상호작용에서의 안정성 측면에서 모두 뛰어난 성능을 보였다.  
특히, OpenAI의 최신 모델 중 하나인 **o1-1217** 과 비슷한 수준의 결과를 기록했다는 점은 DeepSeek의 접근법이 단순히 대안이 아닌, 실질적 경쟁력을 지닌 모델 설계임을 입증했다.  

# Reinforcement Learning Algorithm

## Group Relative Policy Optimization (GRPO)

**Group Relative Policy Optimization (GRPO)**는 DeepSeek 연구팀이 개발한 강화학습 알고리즘으로, 기존의 PPO(Proximal Policy Optimization) 알고리즘을 개선하여 계산 비용을 낮추고 효율적인 최적화를 달성하도록 설계되었다. 기존 강화학습 알고리즘과 달리 개별적인 Critic 모델을 사용하는 대신 그룹 단위의 기준점(baseline)을 설정하여 안정적이고 효율적인 보상 신호를 계산한다. 이러한 접근법으로 강화학습의 효율성을 현저하게 높였다.

### GRPO 알고리즘의 흐름

GRPO는 사람이 일일이 점수를 매기지 않고도 여러 응답 간의 상대적 비교만으로 모델을 학습시키는 방법이다. 다음의 절차를 반복 수행하여 모델의 응답 품질을 향상시킨다:

1. 질문 생성: 모델에게 하나의 질문(query)을 제공한다.

2. 응답 그룹 생성: 모델이 여러 개의 응답(outputs)을 생성한다.

3. 응답 평가 및 보상 지급: 생성된 응답 중 상대적으로 더 나은 응답들에만 보상을 지급한다.

4. 반복 학습: 이 과정을 반복하여 모델이 점진적으로 더 우수한 응답을 생성하도록 유도한다.

여기서 중요한 점은 응답의 절대적 평가가 아닌 상대적 우위만 판단하면 된다는 것이다. 즉 비평 모델(critic model)에 대한 고려를 하지 않아도 된다.  

### GRPO 알고리즘 수식

<PostFigure
  src="/images/Group-Relative-Policy-Optimization-structure.png"
  alt="GRPO 구조"
  width="50%"
  caption="그림 1. Group Relative Policy Optimization structure"
/>

<SmartTable
  rows={[
    ["G", "미니배치 크기 (총 샘플 수)"],
    ["i", <>현재 샘플의 인덱스 <MathInline math="1 \\leq i \\leq G" /></>],
    ["q", "입력 질문 (query)"],
    ["o_i", <> <MathInline math="i" />번째 응답 (output)</>],
    ["\\pi_\\theta(o_i \\mid q)", <>
      현재 정책 <MathInline math="\\theta" />에서 응답 <MathInline math="o_i" />의 생성 비율
    </>],
    ["\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)", <>
      이전 정책 <MathInline math="\\theta_{\\text{old}}" />에서 응답 <MathInline math="o_i" />의 생성 비율
    </>],
    ["\\pi_{\\text{ref}}", "참조 정책 (예: 안전성 기준 모델 또는 베이스라인 모델)"],
    ["A_i", "Advantage (보상 - 기준값), 응답의 상대적 우수성을 나타내는 지표"],
    ["\\epsilon", "클리핑 범위 제한값 (예: 0.1, 0.2)"],
    ["\\beta", "KL 패널티의 중요도를 조절하는 하이퍼파라미터"],
    ["\\text{clip}(r, 1 - \\epsilon, 1 + \\epsilon)", <>
      비율 <MathInline math="r" />을 <MathInline math="[1 - \\epsilon, 1 + \\epsilon]" />으로 제한
    </>],
    ["D_{\\text{KL}}(\\pi_\\theta \\|\\| \\pi_{\\text{ref}})", "정책 간 Kullback-Leibler Divergence (분포 차이)"],
  ]}
/>

구체적인 손실 함수는 아래와 같다:

<MathBlock math={`\\frac{1}{G} \\sum_{i=1}^{G} \\left[ \\min \\left( 
\\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, 
\\text{clip}\\left(\\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon\\right)A_i 
\\right) - \\beta D_{KL}(\\pi_\\theta || \\pi_{\\text{ref}}) \\right]`} />

보상을 최대화하는 방향의 손실 함수를 통해 학습이 이루어지며, 전체 흐름은 다음과 같다.

1. 하나의 질문 $q$에 대해 모델이 여러 개의 응답 $o_i$를 생성한다.

2. 생성된 응답들에 대해, 그룹 내에서의 상대적인 우수성을 기반으로 Advantage $A_i$를 계산한다.

3. 각 응답에 대해 현재 정책이 이전 정책보다 얼마나 더 자주 선택하고 있는지를 나타내는 확률 비율 $r_i$를 계산한다.

4. 이 비율 $r_i$와 Advantage $A_i$를 곱하여, 해당 응답을 얼마나 강화할지를 결정한다.

5. 학습이 불안정해지는 것을 방지하기 위해, 비율 $r_i$는 클리핑(clip) 범위로 제한되며, 원래 값과 클리핑된 값 중 작은 쪽을 선택(min)한다.

6. 현재 정책이 참조 정책 <MathInline math="\pi_{\text{ref}}"/> 와 지나치게 멀어지지 않도록, KL 패널티 항을 통해 차이를 조절한다.

7. 위 과정을 통해 얻어진 손실 함수를 최대화하는 방향으로 파라미터 $\theta$를 업데이트한다.

### 보상 모델링 (Reward Modeling)

또한 DeepSeek-R1은 학습 과정에서 신경망 기반의 보상 모델을 사용하지 않고, **규칙 기반**의 명확한 보상 체계를 적용한다.  
이러한 방식은 모델의 복잡성과 계산 비용을 낮추는 동시에, 보상 해킹(reward hacking) 과 같은 부작용을 효과적으로 방지할 수 있다.

**정확도 보상(Accuracy Rewards)**
- 수학 문제, 프로그래밍 문제처럼 정답이 명확한 과제에서 사용된다.
- 모델의 응답이 정답과 일치하면 높은 보상을, 틀리면 낮은 보상을 준다.
- 이 방식은 정확한 피드백이 가능하고, 보상 정의가 명확하기 때문에 모델이 빠르게 수렴할 수 있다.

**형식 보상 (Format Rewards)**
- 모델이 사고 과정과 최종 답변을 구분해서 표현하도록 유도하기 위한 보상이다.
- 응답 안에 `**<think>**`와 `<answer>` 태그를 명시적으로 포함하게 만들고, 이 구조를 따를 때에만 보상을 부여한다.
- 이를 통해 모델은 내부 사고와 최종 응답을 구분짓는 구조적 표현 방식을 학습하게 된다.

DeepSeek-R1은 규칙 기반 보상 정책을 활용해, 복잡한 보상 모델 없이도 고품질 응답과 사고 구조를 효과적으로 학습할 수 있었다.

# Distillation

DeepSeek 연구팀은 DeepSeek-R1의 뛰어난 추론 능력을 소형 모델로 전달하는 **지식 증류(distillation)** 실험을 진행했다. 이를 통해 학습된 추론 능력을 보다 효율적인 소형 모델에 이전할 수 있음을 증명하였다.  
특히, **Qwen과 LLaMA 기반의 1.5B~70B 규모 모델**을 대상으로 진행한 실험에서, 일부 소형 모델들이 **RL 없이 SFT만으로도 기존의 더 큰 모델보다 높은 성능을 달성**하였다.  
이러한 증류 과정은 앞으로 더 가벼운 AI 모델이 뛰어난 추론 능력을 갖추도록 하는 중요한 전략으로 자리 잡을 수 있을 것으로 기대된다.  

# Conclusion 
### 논란 속 혁신, DeepSeek-R1이 남긴 것

DeepSeek-R1은 확실히 **논란의 중심에 있는 모델**이다.  
무단 데이터 수집 논란, 학습 과정의 불투명성, 중국 정치 관련 검열, 사용자 정보의 비공개 전송 등 여러 문제들이 지적되었고, 이로 인해 업계와 학계에서 상당한 논쟁이 벌어졌다.

하지만 그럼에도 불구하고, 이 모델이 보여준 **기술적 시도와 실험적 의미**를 무시할 수는 없다.

DeepSeek-R1은 단순히 성능 수치만 높인 모델이 아니라, **"AI가 스스로 사고 전략을 학습하고 고도화할 수 있는가?"라는 도전적인 질문에 실질적인 답을 제시한 시도**였다.  
특히 DeepSeek-R1이 생성한 reasoning 데이터로 진행한 **지식 증류(distillation)** 실험은, **작은 모델도 고차 추론 능력을 가질 수 있다는 가능성**을 보여줬고, 이는 앞으로 **경량 AI 모델 시대**로 나아가는 데 있어 굉장히 중요한 전환점이 될 수 있다.

또 하나 주목할 점은, **이 모든 실험이 상대적으로 적은 비용으로 이루어졌다는 점**이다. (물론 논란이 있지만)  
수백억 파라미터의 모델을 단기간에 훈련시키고도 OpenAI급 성능을 주장할 수 있었던 건, 기존 방식과는 다른 **효율 중심의 학습 구조**를 시도했기 때문이다.  
논란이 있음에도 불구하고, **비용 대비 성능이라는 측면에서는 분명히 의미 있는 결과**를 만들어낸 셈이다.

결국, DeepSeek-R1은 완벽하지 않지만, **“AI의 자기 진화 가능성”과 “고성능의 대중화”라는 두 축을 동시에 건드린 모델**이라는 점에서 기억될 것이다.  
앞으로 더 투명하고 정제된 방식으로 후속 연구가 이어진다면, 이 모델이 제시한 방향은 **단순한 실험을 넘어서 AI 개발의 새로운 패러다임**으로 자리 잡을 가능성도 충분하다.

### References

- [DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)